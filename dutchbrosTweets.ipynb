{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import squarify\n",
    "# import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import emoji\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "# Gensim stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS as SW\n",
    "\n",
    "# Wordclouds stopwords\n",
    "from wordcloud import STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "# Establish the English Core Web\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data and go from the original tweet to a tweet with no Emoji's and no url's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_tweet</th>\n",
       "      <th>emoji_free_tweet</th>\n",
       "      <th>url_free_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Flynn was fucking railroaded. https://t.co/4aZ...</td>\n",
       "      <td>Flynn was fucking railroaded. https://t.co/4aZ...</td>\n",
       "      <td>Flynn was fucking railroaded.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>President @realDonaldTrump talks about Opening...</td>\n",
       "      <td>President @realDonaldTrump talks about Opening...</td>\n",
       "      <td>President @realDonaldTrump talks about Opening...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Obamagate Comey magic: Flynn calls \"turned up...</td>\n",
       "      <td>#Obamagate Comey magic: Flynn calls \"turned up...</td>\n",
       "      <td>#Obamagate Comey magic: Flynn calls \"turned up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Matt Gaetz predicts President Trump will pardo...</td>\n",
       "      <td>Matt Gaetz predicts President Trump will pardo...</td>\n",
       "      <td>Matt Gaetz predicts President Trump will pardo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We're in the 21st century Joe... https://t.co/...</td>\n",
       "      <td>We're in the 21st century Joe... https://t.co/...</td>\n",
       "      <td>We're in the 21st century Joe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18518</th>\n",
       "      <td>After a great phone call with the @ASUFootball...</td>\n",
       "      <td>After a great phone call with the @ASUFootball...</td>\n",
       "      <td>After a great phone call with the @ASUFootball...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18519</th>\n",
       "      <td>After a great zoom meeting with the @ASUFootba...</td>\n",
       "      <td>After a great zoom meeting with the @ASUFootba...</td>\n",
       "      <td>After a great zoom meeting with the @ASUFootba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18520</th>\n",
       "      <td>Morning ðŸ’— https://t.co/uu7MhenMOf</td>\n",
       "      <td>Morning https://t.co/uu7MhenMOf</td>\n",
       "      <td>Morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18521</th>\n",
       "      <td>Always be careful who you open up to. Only a f...</td>\n",
       "      <td>Always be careful who you open up to. Only a f...</td>\n",
       "      <td>Always be careful who you open up to. Only a f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18522</th>\n",
       "      <td>You gotta keep your shit private not everyone ...</td>\n",
       "      <td>You gotta keep your shit private not everyone ...</td>\n",
       "      <td>You gotta keep your shit private not everyone ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18523 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          original_tweet  \\\n",
       "0      Flynn was fucking railroaded. https://t.co/4aZ...   \n",
       "1      President @realDonaldTrump talks about Opening...   \n",
       "2      #Obamagate Comey magic: Flynn calls \"turned up...   \n",
       "3      Matt Gaetz predicts President Trump will pardo...   \n",
       "4      We're in the 21st century Joe... https://t.co/...   \n",
       "...                                                  ...   \n",
       "18518  After a great phone call with the @ASUFootball...   \n",
       "18519  After a great zoom meeting with the @ASUFootba...   \n",
       "18520                  Morning ðŸ’— https://t.co/uu7MhenMOf   \n",
       "18521  Always be careful who you open up to. Only a f...   \n",
       "18522  You gotta keep your shit private not everyone ...   \n",
       "\n",
       "                                        emoji_free_tweet  \\\n",
       "0      Flynn was fucking railroaded. https://t.co/4aZ...   \n",
       "1      President @realDonaldTrump talks about Opening...   \n",
       "2      #Obamagate Comey magic: Flynn calls \"turned up...   \n",
       "3      Matt Gaetz predicts President Trump will pardo...   \n",
       "4      We're in the 21st century Joe... https://t.co/...   \n",
       "...                                                  ...   \n",
       "18518  After a great phone call with the @ASUFootball...   \n",
       "18519  After a great zoom meeting with the @ASUFootba...   \n",
       "18520                    Morning https://t.co/uu7MhenMOf   \n",
       "18521  Always be careful who you open up to. Only a f...   \n",
       "18522  You gotta keep your shit private not everyone ...   \n",
       "\n",
       "                                          url_free_tweet  \n",
       "0                         Flynn was fucking railroaded.   \n",
       "1      President @realDonaldTrump talks about Opening...  \n",
       "2      #Obamagate Comey magic: Flynn calls \"turned up...  \n",
       "3      Matt Gaetz predicts President Trump will pardo...  \n",
       "4                      We're in the 21st century Joe...   \n",
       "...                                                  ...  \n",
       "18518  After a great phone call with the @ASUFootball...  \n",
       "18519  After a great zoom meeting with the @ASUFootba...  \n",
       "18520                                           Morning   \n",
       "18521  Always be careful who you open up to. Only a f...  \n",
       "18522  You gotta keep your shit private not everyone ...  \n",
       "\n",
       "[18523 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bring in the JSON\n",
    "url = 'https://raw.githubusercontent.com/jacobpad/Labs-Stuff/master/dutchbros_followers.json'\n",
    "r = requests.get(url)\n",
    "df = r.json()\n",
    "\n",
    "# Simple formating on the JSON\n",
    "df = pd.DataFrame(df.values())\n",
    "df = df.rename(columns={0:'original_tweet'})\n",
    "\n",
    "# Make emoji free text\n",
    "# Source: https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n",
    "def give_emoji_free_text(text):\n",
    "    \"\"\"\n",
    "    Removes emoji's from tweets\n",
    "    Accepts:\n",
    "        Text (tweets)\n",
    "    Returns:\n",
    "        Text (emoji free tweets)\n",
    "    \"\"\"\n",
    "    emoji_list = [c for c in text if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "    return clean_text\n",
    "\n",
    "# Apply the function above and get tweets free of emoji's\n",
    "call_emoji_free = lambda x: give_emoji_free_text(x)\n",
    "\n",
    "# Apply `call_emoji_free` which calls the function to remove all emoji's\n",
    "df['emoji_free_tweet'] = df['original_tweet'].apply(call_emoji_free)\n",
    "\n",
    "\n",
    "# Removing url's\n",
    "def remove_url(text):\n",
    "    \"\"\"\n",
    "    Remove URL's\n",
    "    Accepts:\n",
    "        emoji_free_tweet\n",
    "    Returns:\n",
    "        emoji_free_tweet & url_free_tweet\n",
    "    Makes a new column\n",
    "    \"\"\"\n",
    "    # https://www.youtube.com/watch?v=O2onA4r5UaY\n",
    "    pattern = r\"http\\S+\"\n",
    "    tokens = re.sub(pattern, \"\", text)\n",
    "    return tokens\n",
    "\n",
    "# Make new url_free_tweet column by applying the function on emoji_free_tweet\n",
    "df['url_free_tweet'] = df['emoji_free_tweet'].apply(remove_url)\n",
    "\n",
    "# View the dataframe with 3 columns - original_tweet, emoji_free_tweet, url_free_tweet. \n",
    "# url_free_tweet is also emoji free\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize `url_free_tweet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_tweet</th>\n",
       "      <th>emoji_free_tweet</th>\n",
       "      <th>url_free_tweet</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Flynn was fucking railroaded. https://t.co/4aZ...</td>\n",
       "      <td>Flynn was fucking railroaded. https://t.co/4aZ...</td>\n",
       "      <td>Flynn was fucking railroaded.</td>\n",
       "      <td>[flynn, fucking, railroaded.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>President @realDonaldTrump talks about Opening...</td>\n",
       "      <td>President @realDonaldTrump talks about Opening...</td>\n",
       "      <td>President @realDonaldTrump talks about Opening...</td>\n",
       "      <td>[president, @realdonaldtrump, talks, opening, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Obamagate Comey magic: Flynn calls \"turned up...</td>\n",
       "      <td>#Obamagate Comey magic: Flynn calls \"turned up...</td>\n",
       "      <td>#Obamagate Comey magic: Flynn calls \"turned up...</td>\n",
       "      <td>[#obamagate, comey, magic:, flynn, calls, \"tur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Matt Gaetz predicts President Trump will pardo...</td>\n",
       "      <td>Matt Gaetz predicts President Trump will pardo...</td>\n",
       "      <td>Matt Gaetz predicts President Trump will pardo...</td>\n",
       "      <td>[matt, gaetz, predicts, president, trump, pard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We're in the 21st century Joe... https://t.co/...</td>\n",
       "      <td>We're in the 21st century Joe... https://t.co/...</td>\n",
       "      <td>We're in the 21st century Joe...</td>\n",
       "      <td>[we're, 21st, century, joe...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18518</th>\n",
       "      <td>After a great phone call with the @ASUFootball...</td>\n",
       "      <td>After a great phone call with the @ASUFootball...</td>\n",
       "      <td>After a great phone call with the @ASUFootball...</td>\n",
       "      <td>[great, phone, @asufootball, coachâ€™s, humbled,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18519</th>\n",
       "      <td>After a great zoom meeting with the @ASUFootba...</td>\n",
       "      <td>After a great zoom meeting with the @ASUFootba...</td>\n",
       "      <td>After a great zoom meeting with the @ASUFootba...</td>\n",
       "      <td>[great, zoom, meeting, @asufootball, staff,, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18520</th>\n",
       "      <td>Morning ðŸ’— https://t.co/uu7MhenMOf</td>\n",
       "      <td>Morning https://t.co/uu7MhenMOf</td>\n",
       "      <td>Morning</td>\n",
       "      <td>[morning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18521</th>\n",
       "      <td>Always be careful who you open up to. Only a f...</td>\n",
       "      <td>Always be careful who you open up to. Only a f...</td>\n",
       "      <td>Always be careful who you open up to. Only a f...</td>\n",
       "      <td>[careful, open, to., actually, care,, curious.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18522</th>\n",
       "      <td>You gotta keep your shit private not everyone ...</td>\n",
       "      <td>You gotta keep your shit private not everyone ...</td>\n",
       "      <td>You gotta keep your shit private not everyone ...</td>\n",
       "      <td>[gotta, shit, private, happy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18523 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          original_tweet  \\\n",
       "0      Flynn was fucking railroaded. https://t.co/4aZ...   \n",
       "1      President @realDonaldTrump talks about Opening...   \n",
       "2      #Obamagate Comey magic: Flynn calls \"turned up...   \n",
       "3      Matt Gaetz predicts President Trump will pardo...   \n",
       "4      We're in the 21st century Joe... https://t.co/...   \n",
       "...                                                  ...   \n",
       "18518  After a great phone call with the @ASUFootball...   \n",
       "18519  After a great zoom meeting with the @ASUFootba...   \n",
       "18520                  Morning ðŸ’— https://t.co/uu7MhenMOf   \n",
       "18521  Always be careful who you open up to. Only a f...   \n",
       "18522  You gotta keep your shit private not everyone ...   \n",
       "\n",
       "                                        emoji_free_tweet  \\\n",
       "0      Flynn was fucking railroaded. https://t.co/4aZ...   \n",
       "1      President @realDonaldTrump talks about Opening...   \n",
       "2      #Obamagate Comey magic: Flynn calls \"turned up...   \n",
       "3      Matt Gaetz predicts President Trump will pardo...   \n",
       "4      We're in the 21st century Joe... https://t.co/...   \n",
       "...                                                  ...   \n",
       "18518  After a great phone call with the @ASUFootball...   \n",
       "18519  After a great zoom meeting with the @ASUFootba...   \n",
       "18520                    Morning https://t.co/uu7MhenMOf   \n",
       "18521  Always be careful who you open up to. Only a f...   \n",
       "18522  You gotta keep your shit private not everyone ...   \n",
       "\n",
       "                                          url_free_tweet  \\\n",
       "0                         Flynn was fucking railroaded.    \n",
       "1      President @realDonaldTrump talks about Opening...   \n",
       "2      #Obamagate Comey magic: Flynn calls \"turned up...   \n",
       "3      Matt Gaetz predicts President Trump will pardo...   \n",
       "4                      We're in the 21st century Joe...    \n",
       "...                                                  ...   \n",
       "18518  After a great phone call with the @ASUFootball...   \n",
       "18519  After a great zoom meeting with the @ASUFootba...   \n",
       "18520                                           Morning    \n",
       "18521  Always be careful who you open up to. Only a f...   \n",
       "18522  You gotta keep your shit private not everyone ...   \n",
       "\n",
       "                                                  tokens  \n",
       "0                          [flynn, fucking, railroaded.]  \n",
       "1      [president, @realdonaldtrump, talks, opening, ...  \n",
       "2      [#obamagate, comey, magic:, flynn, calls, \"tur...  \n",
       "3      [matt, gaetz, predicts, president, trump, pard...  \n",
       "4                         [we're, 21st, century, joe...]  \n",
       "...                                                  ...  \n",
       "18518  [great, phone, @asufootball, coachâ€™s, humbled,...  \n",
       "18519  [great, zoom, meeting, @asufootball, staff,, e...  \n",
       "18520                                          [morning]  \n",
       "18521    [careful, open, to., actually, care,, curious.]  \n",
       "18522                      [gotta, shit, private, happy]  \n",
       "\n",
       "[18523 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Import Gensim and Wordcloud to use their stopwords as well and use the combined stopwords of ALL as the variable:\n",
    "ALL_STOP_WORDS\n",
    "\"\"\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "\n",
    "# Custom stopwords\n",
    "custom_stopwords = ['hi','\\n','\\n\\n', '&amp;', ' ', '.', '-', 'got', \"it's\", 'itâ€™s', \"i'm\", 'iâ€™m', 'im', 'want', 'like']\n",
    "\n",
    "# Customize stop words by adding to the default list\n",
    "STOP_WORDS = nlp.Defaults.stop_words.union(custom_stopwords)\n",
    "\n",
    "# ALL_STOP_WORDS = spacy + gensim + wordcloud\n",
    "ALL_STOP_WORDS = STOP_WORDS.union(SW).union(stopwords)\n",
    "\n",
    "\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for doc in tokenizer.pipe(df['url_free_tweet'], batch_size=500):\n",
    "    doc_tokens = []    \n",
    "    for token in doc: \n",
    "        if token.text.lower() not in STOP_WORDS:\n",
    "            doc_tokens.append(token.text.lower())   \n",
    "    tokens.append(doc_tokens)\n",
    "\n",
    "# Makes tokens column\n",
    "df['tokens'] = tokens\n",
    "\n",
    "# View df\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tokens a string again\n",
    "# credit : https://stackoverflow.com/questions/45306988/column-of-lists-convert-list-to-string-as-a-new-column\n",
    "df['tokens_back_to_text'] = [' '.join(map(str, l)) for l in df['tokens']]\n",
    "\n",
    "def get_lemmas(text):\n",
    "    '''Used to lemmatize the processed tweets'''\n",
    "    lemmas = []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Something goes here :P\n",
    "    for token in doc: \n",
    "        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_ != 'PRON'):\n",
    "            lemmas.append(token.lemma_)\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "df['lemmas'] = df['tokens_back_to_text'].apply(get_lemmas)\n",
    "\n",
    "# Make lemmas a string again\n",
    "df['lemmas_back_to_text'] = [' '.join(map(str, l)) for l in df['lemmas']]\n",
    "# df[['original_tweet', 'lemmas_back_to_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the lemmetized text to see what changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_tweet</th>\n",
       "      <th>emoji_free_tweet</th>\n",
       "      <th>url_free_tweet</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_back_to_text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>lemmas_back_to_text</th>\n",
       "      <th>lemma_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Flynn was fucking railroaded. https://t.co/4aZ...</td>\n",
       "      <td>Flynn was fucking railroaded. https://t.co/4aZ...</td>\n",
       "      <td>Flynn was fucking railroaded.</td>\n",
       "      <td>[flynn, fucking, railroaded.]</td>\n",
       "      <td>flynn fucking railroaded.</td>\n",
       "      <td>[flynn, fucking, railroaded]</td>\n",
       "      <td>flynn fucking railroaded</td>\n",
       "      <td>[flynn, fucking, railroaded]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>President @realDonaldTrump talks about Opening...</td>\n",
       "      <td>President @realDonaldTrump talks about Opening...</td>\n",
       "      <td>President @realDonaldTrump talks about Opening...</td>\n",
       "      <td>[president, @realdonaldtrump, talks, opening, ...</td>\n",
       "      <td>president @realdonaldtrump talks opening ameri...</td>\n",
       "      <td>[president, @realdonaldtrump, talks, opening, ...</td>\n",
       "      <td>president @realdonaldtrump talks opening america</td>\n",
       "      <td>[president, @realdonaldtrump, talks, opening, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Obamagate Comey magic: Flynn calls \"turned up...</td>\n",
       "      <td>#Obamagate Comey magic: Flynn calls \"turned up...</td>\n",
       "      <td>#Obamagate Comey magic: Flynn calls \"turned up...</td>\n",
       "      <td>[#obamagate, comey, magic:, flynn, calls, \"tur...</td>\n",
       "      <td>#obamagate comey magic: flynn calls \"turned up...</td>\n",
       "      <td>[obamagate, comey, magic, flynn, call, turn, f...</td>\n",
       "      <td>obamagate comey magic flynn call turn find ova...</td>\n",
       "      <td>[obamagate, comey, magic, flynn, call, turn, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Matt Gaetz predicts President Trump will pardo...</td>\n",
       "      <td>Matt Gaetz predicts President Trump will pardo...</td>\n",
       "      <td>Matt Gaetz predicts President Trump will pardo...</td>\n",
       "      <td>[matt, gaetz, predicts, president, trump, pard...</td>\n",
       "      <td>matt gaetz predicts president trump pardon rog...</td>\n",
       "      <td>[matt, gaetz, predict, president, trump, pardo...</td>\n",
       "      <td>matt gaetz predict president trump pardon roge...</td>\n",
       "      <td>[matt, gaetz, predict, president, trump, pardo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We're in the 21st century Joe... https://t.co/...</td>\n",
       "      <td>We're in the 21st century Joe... https://t.co/...</td>\n",
       "      <td>We're in the 21st century Joe...</td>\n",
       "      <td>[we're, 21st, century, joe...]</td>\n",
       "      <td>we're 21st century joe...</td>\n",
       "      <td>[21st, century, joe]</td>\n",
       "      <td>21st century joe</td>\n",
       "      <td>[century, joe]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18518</th>\n",
       "      <td>After a great phone call with the @ASUFootball...</td>\n",
       "      <td>After a great phone call with the @ASUFootball...</td>\n",
       "      <td>After a great phone call with the @ASUFootball...</td>\n",
       "      <td>[great, phone, @asufootball, coachâ€™s, humbled,...</td>\n",
       "      <td>great phone @asufootball coachâ€™s humbled bless...</td>\n",
       "      <td>[great, phone, @asufootball, coach, humble, bl...</td>\n",
       "      <td>great phone @asufootball coach humble blessed ...</td>\n",
       "      <td>[great, phone, @asufootball, coach, humble, bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18519</th>\n",
       "      <td>After a great zoom meeting with the @ASUFootba...</td>\n",
       "      <td>After a great zoom meeting with the @ASUFootba...</td>\n",
       "      <td>After a great zoom meeting with the @ASUFootba...</td>\n",
       "      <td>[great, zoom, meeting, @asufootball, staff,, e...</td>\n",
       "      <td>great zoom meeting @asufootball staff, extreme...</td>\n",
       "      <td>[great, zoom, meeting, @asufootball, staff, ex...</td>\n",
       "      <td>great zoom meeting @asufootball staff extremel...</td>\n",
       "      <td>[great, zoom, meeting, @asufootball, staff, ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18520</th>\n",
       "      <td>Morning ðŸ’— https://t.co/uu7MhenMOf</td>\n",
       "      <td>Morning https://t.co/uu7MhenMOf</td>\n",
       "      <td>Morning</td>\n",
       "      <td>[morning]</td>\n",
       "      <td>morning</td>\n",
       "      <td>[morning]</td>\n",
       "      <td>morning</td>\n",
       "      <td>[morning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18521</th>\n",
       "      <td>Always be careful who you open up to. Only a f...</td>\n",
       "      <td>Always be careful who you open up to. Only a f...</td>\n",
       "      <td>Always be careful who you open up to. Only a f...</td>\n",
       "      <td>[careful, open, to., actually, care,, curious.]</td>\n",
       "      <td>careful open to. actually care, curious.</td>\n",
       "      <td>[careful, open, actually, care, curious]</td>\n",
       "      <td>careful open actually care curious</td>\n",
       "      <td>[careful, open, actually, care, curious]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18522</th>\n",
       "      <td>You gotta keep your shit private not everyone ...</td>\n",
       "      <td>You gotta keep your shit private not everyone ...</td>\n",
       "      <td>You gotta keep your shit private not everyone ...</td>\n",
       "      <td>[gotta, shit, private, happy]</td>\n",
       "      <td>gotta shit private happy</td>\n",
       "      <td>[get, to, shit, private, happy]</td>\n",
       "      <td>get to shit private happy</td>\n",
       "      <td>[get, to, shit, private, happy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18523 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          original_tweet  \\\n",
       "0      Flynn was fucking railroaded. https://t.co/4aZ...   \n",
       "1      President @realDonaldTrump talks about Opening...   \n",
       "2      #Obamagate Comey magic: Flynn calls \"turned up...   \n",
       "3      Matt Gaetz predicts President Trump will pardo...   \n",
       "4      We're in the 21st century Joe... https://t.co/...   \n",
       "...                                                  ...   \n",
       "18518  After a great phone call with the @ASUFootball...   \n",
       "18519  After a great zoom meeting with the @ASUFootba...   \n",
       "18520                  Morning ðŸ’— https://t.co/uu7MhenMOf   \n",
       "18521  Always be careful who you open up to. Only a f...   \n",
       "18522  You gotta keep your shit private not everyone ...   \n",
       "\n",
       "                                        emoji_free_tweet  \\\n",
       "0      Flynn was fucking railroaded. https://t.co/4aZ...   \n",
       "1      President @realDonaldTrump talks about Opening...   \n",
       "2      #Obamagate Comey magic: Flynn calls \"turned up...   \n",
       "3      Matt Gaetz predicts President Trump will pardo...   \n",
       "4      We're in the 21st century Joe... https://t.co/...   \n",
       "...                                                  ...   \n",
       "18518  After a great phone call with the @ASUFootball...   \n",
       "18519  After a great zoom meeting with the @ASUFootba...   \n",
       "18520                    Morning https://t.co/uu7MhenMOf   \n",
       "18521  Always be careful who you open up to. Only a f...   \n",
       "18522  You gotta keep your shit private not everyone ...   \n",
       "\n",
       "                                          url_free_tweet  \\\n",
       "0                         Flynn was fucking railroaded.    \n",
       "1      President @realDonaldTrump talks about Opening...   \n",
       "2      #Obamagate Comey magic: Flynn calls \"turned up...   \n",
       "3      Matt Gaetz predicts President Trump will pardo...   \n",
       "4                      We're in the 21st century Joe...    \n",
       "...                                                  ...   \n",
       "18518  After a great phone call with the @ASUFootball...   \n",
       "18519  After a great zoom meeting with the @ASUFootba...   \n",
       "18520                                           Morning    \n",
       "18521  Always be careful who you open up to. Only a f...   \n",
       "18522  You gotta keep your shit private not everyone ...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0                          [flynn, fucking, railroaded.]   \n",
       "1      [president, @realdonaldtrump, talks, opening, ...   \n",
       "2      [#obamagate, comey, magic:, flynn, calls, \"tur...   \n",
       "3      [matt, gaetz, predicts, president, trump, pard...   \n",
       "4                         [we're, 21st, century, joe...]   \n",
       "...                                                  ...   \n",
       "18518  [great, phone, @asufootball, coachâ€™s, humbled,...   \n",
       "18519  [great, zoom, meeting, @asufootball, staff,, e...   \n",
       "18520                                          [morning]   \n",
       "18521    [careful, open, to., actually, care,, curious.]   \n",
       "18522                      [gotta, shit, private, happy]   \n",
       "\n",
       "                                     tokens_back_to_text  \\\n",
       "0                              flynn fucking railroaded.   \n",
       "1      president @realdonaldtrump talks opening ameri...   \n",
       "2      #obamagate comey magic: flynn calls \"turned up...   \n",
       "3      matt gaetz predicts president trump pardon rog...   \n",
       "4                              we're 21st century joe...   \n",
       "...                                                  ...   \n",
       "18518  great phone @asufootball coachâ€™s humbled bless...   \n",
       "18519  great zoom meeting @asufootball staff, extreme...   \n",
       "18520                                            morning   \n",
       "18521           careful open to. actually care, curious.   \n",
       "18522                           gotta shit private happy   \n",
       "\n",
       "                                                  lemmas  \\\n",
       "0                           [flynn, fucking, railroaded]   \n",
       "1      [president, @realdonaldtrump, talks, opening, ...   \n",
       "2      [obamagate, comey, magic, flynn, call, turn, f...   \n",
       "3      [matt, gaetz, predict, president, trump, pardo...   \n",
       "4                                   [21st, century, joe]   \n",
       "...                                                  ...   \n",
       "18518  [great, phone, @asufootball, coach, humble, bl...   \n",
       "18519  [great, zoom, meeting, @asufootball, staff, ex...   \n",
       "18520                                          [morning]   \n",
       "18521           [careful, open, actually, care, curious]   \n",
       "18522                    [get, to, shit, private, happy]   \n",
       "\n",
       "                                     lemmas_back_to_text  \\\n",
       "0                               flynn fucking railroaded   \n",
       "1       president @realdonaldtrump talks opening america   \n",
       "2      obamagate comey magic flynn call turn find ova...   \n",
       "3      matt gaetz predict president trump pardon roge...   \n",
       "4                                       21st century joe   \n",
       "...                                                  ...   \n",
       "18518  great phone @asufootball coach humble blessed ...   \n",
       "18519  great zoom meeting @asufootball staff extremel...   \n",
       "18520                                            morning   \n",
       "18521                 careful open actually care curious   \n",
       "18522                          get to shit private happy   \n",
       "\n",
       "                                            lemma_tokens  \n",
       "0                           [flynn, fucking, railroaded]  \n",
       "1      [president, @realdonaldtrump, talks, opening, ...  \n",
       "2      [obamagate, comey, magic, flynn, call, turn, f...  \n",
       "3      [matt, gaetz, predict, president, trump, pardo...  \n",
       "4                                         [century, joe]  \n",
       "...                                                  ...  \n",
       "18518  [great, phone, @asufootball, coach, humble, bl...  \n",
       "18519  [great, zoom, meeting, @asufootball, staff, ex...  \n",
       "18520                                          [morning]  \n",
       "18521           [careful, open, actually, care, curious]  \n",
       "18522                    [get, to, shit, private, happy]  \n",
       "\n",
       "[18523 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "# Tokenizer function\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Parses a string into a list of semantic units (words)\n",
    "    Args:\n",
    "        text (str): The string that the function will tokenize.\n",
    "    Returns:\n",
    "        list: tokens parsed out\n",
    "    \"\"\"\n",
    "    # Removing url's\n",
    "    pattern = r\"http\\S+\"\n",
    "    \n",
    "    tokens = re.sub(pattern, \"\", text) # https://www.youtube.com/watch?v=O2onA4r5UaY\n",
    "    tokens = re.sub('[^a-zA-Z 0-9]', '', text)\n",
    "    tokens = re.sub('[%s]' % re.escape(string.punctuation), '', text) # Remove punctuation\n",
    "    tokens = re.sub('\\w*\\d\\w*', '', text) # Remove words containing numbers\n",
    "    tokens = tokens.strip(',') # TESTING THIS LINE\n",
    "    tokens = tokens.strip('?') # TESTING THIS LINE\n",
    "    tokens = tokens.strip('!') # TESTING THIS LINE\n",
    "    tokens = tokens.strip(\"'\") # TESTING THIS LINE\n",
    "    tokens = tokens.strip(\".\") # TESTING THIS LINE\n",
    "\n",
    "    tokens = tokens.lower().split() # Make text lowercase and split it\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply tokenizer\n",
    "df['lemma_tokens'] = df['lemmas_back_to_text'].apply(tokenize)\n",
    "\n",
    "# View those tokens (the 4th column)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>appears_in</th>\n",
       "      <th>count</th>\n",
       "      <th>rank</th>\n",
       "      <th>pct_total</th>\n",
       "      <th>cul_pct_total</th>\n",
       "      <th>appears_in_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>people</td>\n",
       "      <td>910</td>\n",
       "      <td>1033</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006728</td>\n",
       "      <td>0.006728</td>\n",
       "      <td>0.049128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>good</td>\n",
       "      <td>857</td>\n",
       "      <td>928</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.006044</td>\n",
       "      <td>0.012772</td>\n",
       "      <td>0.046267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>time</td>\n",
       "      <td>780</td>\n",
       "      <td>844</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.005497</td>\n",
       "      <td>0.018269</td>\n",
       "      <td>0.042110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>love</td>\n",
       "      <td>751</td>\n",
       "      <td>830</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.023674</td>\n",
       "      <td>0.040544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>know</td>\n",
       "      <td>729</td>\n",
       "      <td>765</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.004982</td>\n",
       "      <td>0.028657</td>\n",
       "      <td>0.039356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  appears_in  count  rank  pct_total  cul_pct_total  appears_in_pct\n",
       "188  people         910   1033   1.0   0.006728       0.006728        0.049128\n",
       "283    good         857    928   2.0   0.006044       0.012772        0.046267\n",
       "47     time         780    844   3.0   0.005497       0.018269        0.042110\n",
       "267    love         751    830   4.0   0.005406       0.023674        0.040544\n",
       "276    know         729    765   5.0   0.004982       0.028657        0.039356"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count Function to count tokens\n",
    "def count(docs):\n",
    "    word_counts = Counter()\n",
    "    appears_in = Counter()\n",
    "        \n",
    "    total_docs = len(docs)\n",
    "\n",
    "    for doc in docs:\n",
    "        word_counts.update(doc)\n",
    "        appears_in.update(set(doc))\n",
    "\n",
    "    temp = zip(word_counts.keys(), word_counts.values())\n",
    "        \n",
    "    wc = pd.DataFrame(temp, columns = ['word', 'count'])\n",
    "\n",
    "    wc['rank'] = wc['count'].rank(method='first', ascending=False)\n",
    "    total = wc['count'].sum()\n",
    "\n",
    "    wc['pct_total'] = wc['count'].apply(lambda x: x / total)\n",
    "        \n",
    "    wc = wc.sort_values(by='rank')\n",
    "    wc['cul_pct_total'] = wc['pct_total'].cumsum()\n",
    "\n",
    "    t2 = zip(appears_in.keys(), appears_in.values())\n",
    "    ac = pd.DataFrame(t2, columns=['word', 'appears_in'])\n",
    "    wc = ac.merge(wc, on='word')\n",
    "\n",
    "    wc['appears_in_pct'] = wc['appears_in'].apply(lambda x: x / total_docs)\n",
    "        \n",
    "    return wc.sort_values(by='rank')\n",
    "\n",
    "# The object `Counter` takes an iterable, but you can instaniate an empty one and update it. \n",
    "word_counts = Counter()\n",
    "\n",
    "\n",
    "wc = count(df['lemmas'])\n",
    "wc.head(20)\n",
    "tokens = []\n",
    "\n",
    "for doc in tokenizer.pipe(df['lemmas_back_to_text'], batch_size=500):\n",
    "    doc_tokens = []    \n",
    "    for token in doc: \n",
    "        if token.text.lower() not in STOP_WORDS:\n",
    "            doc_tokens.append(token.text.lower())   \n",
    "    tokens.append(doc_tokens)\n",
    "    \n",
    "df['lemmas'] = tokens\n",
    "\n",
    "wc = count(df['lemmas'])\n",
    "wc.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elon_tweets",
   "language": "python",
   "name": "elon-tweets"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
